apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: homelab-alerts
  namespace: monitoring
  labels:
    release: kube-prometheus-stack
spec:
  groups:
    - name: homelab-node
      rules:
        - alert: NodeHighCPU
          expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage on {{ $labels.instance }}"
            description: "CPU usage is above 85% for 10 minutes (current: {{ $value | printf \"%.1f\" }}%)"
        - alert: NodeHighMemory
          expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage on {{ $labels.instance }}"
            description: "Memory usage is above 90% for 10 minutes (current: {{ $value | printf \"%.1f\" }}%)"
        - alert: NodeFilesystemAlmostFull
          expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100 > 85
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Filesystem almost full on {{ $labels.instance }}"
            description: "{{ $labels.mountpoint }} is {{ $value | printf \"%.1f\" }}% full"
        - alert: NodeFilesystemCritical
          expr: (1 - node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100 > 95
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Filesystem critically full on {{ $labels.instance }}"
            description: "{{ $labels.mountpoint }} is {{ $value | printf \"%.1f\" }}% full"

    - name: homelab-pods
      rules:
        - alert: PodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            description: "Container {{ $labels.container }} has restarted {{ $value | printf \"%.0f\" }} times in the last hour"
        - alert: PodNotReady
          expr: kube_pod_status_ready{condition="true"} == 0 and on(pod, namespace) kube_pod_status_phase{phase="Running"} == 1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"
            description: "Pod has been running but not ready for 15 minutes"
        - alert: DeploymentReplicasMismatch
          expr: kube_deployment_spec_replicas != kube_deployment_status_ready_replicas
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
            description: "Expected {{ $value }} replicas but not all are ready"

    - name: homelab-storage
      rules:
        - alert: PVCAlmostFull
          expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} almost full"
            description: "PVC is {{ $value | printf \"%.1f\" }}% full"
        - alert: PVCCriticallyFull
          expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 95
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} critically full"
            description: "PVC is {{ $value | printf \"%.1f\" }}% full"
        - alert: PVFailed
          expr: kube_persistentvolume_status_phase{phase="Failed"} == 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "PV {{ $labels.persistentvolume }} is in Failed state"
            description: "Persistent Volume has entered a Failed phase"

    - name: homelab-argocd
      rules:
        - alert: ArgoCDAppOutOfSync
          expr: argocd_app_info{sync_status!="Synced"} == 1
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: "ArgoCD app {{ $labels.name }} is out of sync"
            description: "Application has been out of sync for 30 minutes (status: {{ $labels.sync_status }})"
        - alert: ArgoCDAppDegraded
          expr: argocd_app_info{health_status=~"Degraded|Missing"} == 1
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "ArgoCD app {{ $labels.name }} is {{ $labels.health_status }}"
            description: "Application health is {{ $labels.health_status }} for 15 minutes"

    - name: homelab-longhorn
      rules:
        - alert: LonghornVolumedegraded
          expr: longhorn_volume_robustness == 2
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Longhorn volume {{ $labels.volume }} is degraded"
            description: "Volume has lost replica redundancy"
        - alert: LonghornVolumeFaulted
          expr: longhorn_volume_robustness == 3
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Longhorn volume {{ $labels.volume }} is faulted"
            description: "Volume is faulted and may be unavailable"
        - alert: LonghornNodeStorageHigh
          expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 80
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Longhorn storage high on {{ $labels.node }}"
            description: "Node storage usage is {{ $value | printf \"%.1f\" }}%"
        - alert: LonghornNodeStorageCritical
          expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 > 90
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Longhorn storage critical on {{ $labels.node }}"
            description: "Node storage usage is {{ $value | printf \"%.1f\" }}%"
        - alert: LonghornDiskUnhealthy
          expr: longhorn_disk_status == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Longhorn disk unhealthy on {{ $labels.node }}"
            description: "Disk {{ $labels.disk }} is not schedulable or unhealthy"
        - alert: LonghornVolumeHighLatency
          expr: longhorn_volume_write_latency > 5000000
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Longhorn volume {{ $labels.volume }} has high write latency"
            description: "Write latency is {{ $value | humanize }}ns"

    - name: homelab-traefik
      rules:
        - alert: TraefikHighErrorRate
          expr: sum(rate(traefik_service_requests_total{code=~"5.."}[5m])) / sum(rate(traefik_service_requests_total[5m])) * 100 > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Traefik high 5xx error rate"
            description: "{{ $value | printf \"%.1f\" }}% of requests are returning 5xx errors"
        - alert: TraefikServiceHighErrorRate
          expr: sum by (service) (rate(traefik_service_requests_total{code=~"5.."}[5m])) / sum by (service) (rate(traefik_service_requests_total[5m])) * 100 > 10
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Traefik service {{ $labels.service }} high error rate"
            description: "{{ $value | printf \"%.1f\" }}% of requests to {{ $labels.service }} are 5xx"
        - alert: TraefikConfigReloadFailure
          expr: traefik_config_last_reload_success == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Traefik config reload failed"
            description: "Traefik's last configuration reload was not successful"
        - alert: TraefikHighRequestLatency
          expr: histogram_quantile(0.95, sum(rate(traefik_service_request_duration_seconds_bucket[5m])) by (le, service)) > 3
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Traefik service {{ $labels.service }} high latency"
            description: "P95 latency is {{ $value | printf \"%.2f\" }}s"

    - name: homelab-synology-csi
      rules:
        - alert: SynologyCSIControllerDown
          expr: absent(kube_pod_container_status_running{namespace="synology-csi", pod=~"synology-csi-controller.*", container="csi-plugin"} == 1)
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Synology CSI controller is down"
            description: "The CSI controller pod is not running"
        - alert: SynologyCSINodePluginDown
          expr: count(kube_pod_container_status_running{namespace="synology-csi", pod=~"synology-csi-node.*", container="csi-plugin"} == 1) < count(kube_node_info)
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Synology CSI node plugin not running on all nodes"
            description: "Only {{ $value }} node plugins running, expected {{ with query \"count(kube_node_info)\" }}{{ . | first | value }}{{ end }}"
        - alert: SynologyCSIPodRestarting
          expr: increase(kube_pod_container_status_restarts_total{namespace="synology-csi"}[1h]) > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Synology CSI pod {{ $labels.pod }} is restarting"
            description: "Container {{ $labels.container }} has restarted {{ $value | printf \"%.0f\" }} times in the last hour"
        - alert: SynologyISCSIPVFailed
          expr: kube_persistentvolume_status_phase{phase="Failed", storageclass="synology-iscsi"} == 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Synology iSCSI PV {{ $labels.persistentvolume }} failed"
            description: "A Synology iSCSI persistent volume has entered Failed state"

    - name: homelab-cert-manager
      rules:
        - alert: CertManagerCertExpiringSoon
          expr: certmanager_certificate_expiration_timestamp_seconds - time() < 21 * 24 * 3600
          for: 1h
          labels:
            severity: warning
          annotations:
            summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expiring soon"
            description: "Certificate expires in {{ $value | humanizeDuration }}"
        - alert: CertManagerCertExpiryCritical
          expr: certmanager_certificate_expiration_timestamp_seconds - time() < 7 * 24 * 3600
          for: 1h
          labels:
            severity: critical
          annotations:
            summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} expiring in less than 7 days"
            description: "Certificate expires in {{ $value | humanizeDuration }}"
        - alert: CertManagerCertNotReady
          expr: certmanager_certificate_ready_status{condition="True"} == 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Certificate {{ $labels.namespace }}/{{ $labels.name }} is not ready"
            description: "Certificate has been in a not-ready state for 15 minutes"
        - alert: CertManagerSyncErrors
          expr: rate(certmanager_controller_sync_error_count[5m]) > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "cert-manager controller sync errors"
            description: "cert-manager is experiencing sync errors for 10 minutes"

    - name: homelab-cloudflared
      rules:
        - alert: CloudflaredTunnelDown
          expr: cloudflared_tunnel_ha_connections == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Cloudflared tunnel has no active connections"
            description: "All HA connections to Cloudflare edge are down — external access is broken"
        - alert: CloudflaredTunnelDegraded
          expr: cloudflared_tunnel_ha_connections < 2
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Cloudflared tunnel has reduced HA connections"
            description: "Only {{ $value }} HA connection(s) active (expected >=2)"
        - alert: CloudflaredHighErrorRate
          expr: rate(cloudflared_tunnel_request_errors[5m]) / rate(cloudflared_tunnel_total_requests[5m]) * 100 > 5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Cloudflared tunnel high error rate"
            description: "{{ $value | printf \"%.1f\" }}% of tunnel requests are failing"
        - alert: CloudflaredHighConnectLatency
          expr: histogram_quantile(0.95, rate(cloudflared_proxy_connect_latency_bucket[5m])) > 1
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Cloudflared high proxy connect latency"
            description: "P95 connect latency is {{ $value | printf \"%.2f\" }}s"

    - name: homelab-external-dns
      rules:
        - alert: ExternalDNSSyncErrors
          expr: increase(external_dns_controller_consecutive_soft_errors[5m]) > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "external-dns is experiencing sync errors"
            description: "Consecutive soft errors detected for 15 minutes"
        - alert: ExternalDNSStaleSync
          expr: time() - external_dns_controller_last_sync_timestamp_seconds > 900
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "external-dns has not synced recently"
            description: "Last sync was {{ $value | humanizeDuration }} ago (threshold: 15m)"

    - name: homelab-sealed-secrets
      rules:
        - alert: SealedSecretsUnsealErrors
          expr: increase(sealed_secrets_controller_unseal_errors_total[1h]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Sealed Secrets controller unseal errors"
            description: "{{ $value | printf \"%.0f\" }} unseal error(s) in the last hour"

    - name: homelab-immich-ml
      rules:
        - alert: ImmichMLDown
          expr: kube_deployment_status_available_replicas{namespace="immich-ml", deployment="immich-ml"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Immich ML is down"
            description: "No available replicas for Immich ML — Smart Search and Face Detection will fail"
        - alert: ImmichMLCrashLooping
          expr: increase(kube_pod_container_status_restarts_total{namespace="immich-ml"}[1h]) > 3
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Immich ML pod {{ $labels.pod }} is crash looping"
            description: "Container {{ $labels.container }} has restarted {{ $value | printf \"%.0f\" }} times in the last hour"
        - alert: ImmichMLHighMemory
          expr: container_memory_working_set_bytes{namespace="immich-ml", container="immich-ml"} > 4 * 1024^3
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Immich ML using excessive memory"
            description: "Memory usage is {{ $value | humanize1024 }}B (threshold: 4Gi)"

    - name: homelab-flaresolverr
      rules:
        - alert: FlareSolverrDown
          expr: up{job="flaresolverr", namespace="flaresolverr"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "FlareSolverr is down"
            description: "FlareSolverr metrics endpoint is unreachable for 5 minutes"
        - alert: FlareSolverrHighFailureRate
          expr: sum(rate(flaresolverr_request_total{namespace="flaresolverr", result!="solved"}[15m])) / sum(rate(flaresolverr_request_total{namespace="flaresolverr"}[15m])) * 100 > 50
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "FlareSolverr high failure rate"
            description: "{{ $value | printf \"%.0f\" }}% of requests are failing (threshold: 50%)"
        - alert: FlareSolverrSlowRequests
          expr: histogram_quantile(0.95, sum by (le) (rate(flaresolverr_request_duration_bucket{namespace="flaresolverr"}[15m]))) > 120
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "FlareSolverr requests are slow"
            description: "P95 request duration is {{ $value | printf \"%.1f\" }}s (threshold: 120s)"

    - name: homelab-sonarr
      rules:
        - alert: SonarrDown
          expr: up{job="sonarr", namespace="sonarr"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Sonarr is down"
            description: "Sonarr metrics endpoint is unreachable for 5 minutes"
        - alert: SonarrHealthIssues
          expr: sonarr_system_health_issues{namespace="sonarr"} > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Sonarr has health issues"
            description: "Sonarr reports {{ $value }} health issue(s)"
        - alert: SonarrQueueStuck
          expr: sonarr_queue_total{namespace="sonarr"} > 0
          for: 6h
          labels:
            severity: warning
          annotations:
            summary: "Sonarr queue is stuck"
            description: "{{ $value }} item(s) in queue for over 6 hours"

    - name: homelab-radarr
      rules:
        - alert: RadarrDown
          expr: up{job="radarr", namespace="radarr"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Radarr is down"
            description: "Radarr metrics endpoint is unreachable for 5 minutes"
        - alert: RadarrHealthIssues
          expr: radarr_system_health_issues{namespace="radarr"} > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: "Radarr has health issues"
            description: "Radarr reports {{ $value }} health issue(s)"
        - alert: RadarrQueueStuck
          expr: radarr_queue_total{namespace="radarr"} > 0
          for: 6h
          labels:
            severity: warning
          annotations:
            summary: "Radarr queue is stuck"
            description: "{{ $value }} item(s) in queue for over 6 hours"
