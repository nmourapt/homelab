apiVersion: batch/v1
kind: Job
metadata:
  name: es-cloudflare-logpush-setup
  namespace: elastic-system
spec:
  backoffLimit: 10
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: setup
          image: curlimages/curl:8.12.1
          command:
            - /bin/sh
            - -c
            - |
              KIBANA="https://kibana-kb-http.elastic-system.svc:5601"
              AUTH="elastic:${ES_PASSWORD}"
              BUCKET="logs"

              # Wait for Kibana
              until curl -sk -u "${AUTH}" "${KIBANA}/api/status" | grep -q '"overall"'; do
                echo "Waiting for Kibana..."
                sleep 10
              done
              echo "Kibana is ready"

              # Install cloudflare_logpush package
              echo "Installing cloudflare_logpush package..."
              curl -sk -u "${AUTH}" -X POST "${KIBANA}/api/fleet/epm/packages/cloudflare_logpush" \
                -H "kbn-xsrf: true" -H "Content-Type: application/json" \
                -d '{"force": true}'
              echo ""
              sleep 10

              # Format: dataset_name:r2_folder_name
              # dataset_name = package policy_template + data_stream dataset
              # r2_folder_name = actual folder prefix in the R2 bucket
              STREAM_MAP="
              access_request:access_request
              audit:audit_logs
              device_posture:device_posture
              dns:dns
              firewall_event:firewall_event
              gateway_dns:gateway_dns
              gateway_http:gateway_http
              gateway_network:gateway_network
              http_request:http_request
              network_analytics:network_analytics_logs
              network_session:network_session
              spectrum_event:spectrum_event
              workers_trace:workers_trace
              "

              # Build inputs array
              # Input-level vars: collect_s3_logs, cloudflare_r2, access_key_id, secret_access_key, endpoint
              # Stream-level vars: cloudflare_r2_<dataset>, bucket_list_prefix, and required bools
              INPUTS="["
              FIRST=true
              for ENTRY in ${STREAM_MAP}; do
                DATASET=$(echo "${ENTRY}" | cut -d: -f1)
                FOLDER=$(echo "${ENTRY}" | cut -d: -f2)
                if [ "${FIRST}" = "true" ]; then
                  FIRST=false
                else
                  INPUTS="${INPUTS},"
                fi
                INPUTS="${INPUTS}{
                  \"type\": \"aws-s3\",
                  \"policy_template\": \"${DATASET}\",
                  \"enabled\": true,
                  \"vars\": {
                    \"collect_s3_logs\": {\"value\": true, \"type\": \"bool\"},
                    \"cloudflare_r2\": {\"value\": \"${BUCKET}\", \"type\": \"text\"},
                    \"access_key_id\": {\"value\": \"${R2_ACCESS_KEY_ID}\", \"type\": \"password\"},
                    \"secret_access_key\": {\"value\": \"${R2_SECRET_ACCESS_KEY}\", \"type\": \"password\"},
                    \"endpoint\": {\"value\": \"${R2_ENDPOINT}\", \"type\": \"text\"}
                  },
                  \"streams\": [{
                    \"enabled\": true,
                    \"data_stream\": {
                      \"type\": \"logs\",
                      \"dataset\": \"cloudflare_logpush.${DATASET}\"
                    },
                    \"vars\": {
                      \"cloudflare_r2_${DATASET}\": {\"value\": \"${BUCKET}\", \"type\": \"text\"},
                      \"bucket_list_prefix\": {\"value\": \"${FOLDER}/\", \"type\": \"text\"},
                      \"number_of_workers\": {\"value\": 3, \"type\": \"integer\"},
                      \"interval\": {\"value\": \"1m\", \"type\": \"text\"},
                      \"enable_deduplication\": {\"value\": true, \"type\": \"bool\"},
                      \"preserve_original_event\": {\"value\": false, \"type\": \"bool\"},
                      \"preserve_duplicate_custom_fields\": {\"value\": false, \"type\": \"bool\"},
                      \"tags\": {\"value\": [\"forwarded\", \"cloudflare_logpush-${DATASET}\"], \"type\": \"text\"}
                    }
                  }]
                }"
              done

              # Disable non-S3 inputs for each stream
              for ENTRY in ${STREAM_MAP}; do
                DATASET=$(echo "${ENTRY}" | cut -d: -f1)
                for INPUT_TYPE in httpjson gcs azure-blob-storage; do
                  INPUTS="${INPUTS},{
                    \"type\": \"${INPUT_TYPE}\",
                    \"policy_template\": \"${DATASET}\",
                    \"enabled\": false,
                    \"streams\": [{
                      \"enabled\": false,
                      \"data_stream\": {
                        \"type\": \"logs\",
                        \"dataset\": \"cloudflare_logpush.${DATASET}\"
                      }
                    }]
                  }"
                done
              done
              INPUTS="${INPUTS}]"

              echo "Creating package policy with all 13 data streams..."
              RESULT=$(curl -sk -u "${AUTH}" -X POST "${KIBANA}/api/fleet/package_policies" \
                -H "kbn-xsrf: true" -H "Content-Type: application/json" \
                -d "{
                  \"name\": \"Cloudflare Logpush - R2\",
                  \"namespace\": \"default\",
                  \"policy_id\": \"eck-fleet-server\",
                  \"package\": {
                    \"name\": \"cloudflare_logpush\",
                    \"version\": \"1.43.2\"
                  },
                  \"inputs\": ${INPUTS}
                }")
              echo "${RESULT}" | head -c 500
              echo ""

              if echo "${RESULT}" | grep -q '"item"'; then
                echo "SUCCESS: Cloudflare Logpush integration configured"
              else
                echo "WARN: Check output above for errors"
                echo "${RESULT}" | tail -c 500
              fi
          env:
            - name: ES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elasticsearch-es-elastic-user
                  key: elastic
            - name: R2_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: r2-logpush-credentials
                  key: R2_ACCESS_KEY_ID
            - name: R2_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: r2-logpush-credentials
                  key: R2_SECRET_ACCESS_KEY
            - name: R2_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: r2-logpush-credentials
                  key: R2_ENDPOINT
