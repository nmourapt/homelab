apiVersion: batch/v1
kind: Job
metadata:
  name: es-cloudflare-logpush-setup
  namespace: elastic-system
spec:
  backoffLimit: 10
  template:
    spec:
      restartPolicy: OnFailure
      containers:
        - name: setup
          image: curlimages/curl:8.12.1
          command:
            - /bin/sh
            - -c
            - |
              KIBANA="https://kibana-kb-http.elastic-system.svc:5601"
              AUTH="elastic:${ES_PASSWORD}"
              HEADERS='-H "kbn-xsrf: true" -H "Content-Type: application/json"'

              # Wait for Kibana
              until curl -sk -u "${AUTH}" "${KIBANA}/api/status" | grep -q '"overall"'; do
                echo "Waiting for Kibana..."
                sleep 10
              done
              echo "Kibana is ready"

              # Install cloudflare_logpush package
              echo "Installing cloudflare_logpush package..."
              curl -sk -u "${AUTH}" -X POST "${KIBANA}/api/fleet/epm/packages/cloudflare_logpush" \
                -H "kbn-xsrf: true" -H "Content-Type: application/json" \
                -d '{"force": true}'
              echo ""

              # Wait for package to be installed
              sleep 10

              # S3 common config
              ENDPOINT="https://d94d0ebef658cbd92166d633c8f065c9.r2.cloudflarestorage.com"
              ACCESS_KEY="REDACTED_ACCESS_KEY"
              SECRET_KEY="REDACTED_SECRET_KEY"
              BUCKET="logs"

              # Data streams and their R2 folder prefixes
              STREAMS="access_request audit_logs device_posture dns firewall_event gateway_dns gateway_http gateway_network http_request network_analytics_logs network_session spectrum_event workers_trace"

              # Build the inputs array for the package policy
              # Each data stream needs an S3 input enabled with the correct bucket_list_prefix
              INPUTS="["

              for STREAM in ${STREAMS}; do
                if [ "${INPUTS}" != "[" ]; then
                  INPUTS="${INPUTS},"
                fi
                INPUTS="${INPUTS}{
                  \"type\": \"aws-s3\",
                  \"policy_template\": \"${STREAM}\",
                  \"enabled\": true,
                  \"streams\": [{
                    \"enabled\": true,
                    \"data_stream\": {
                      \"type\": \"logs\",
                      \"dataset\": \"cloudflare_logpush.${STREAM}\"
                    },
                    \"vars\": {
                      \"bucket_list_prefix\": {\"value\": \"${STREAM}/\", \"type\": \"text\"},
                      \"access_key_id\": {\"value\": \"${ACCESS_KEY}\", \"type\": \"text\"},
                      \"secret_access_key\": {\"value\": \"${SECRET_KEY}\", \"type\": \"password\"},
                      \"bucket_arn\": {\"value\": \"arn:aws:s3:::${BUCKET}\", \"type\": \"text\"},
                      \"endpoint\": {\"value\": \"${ENDPOINT}\", \"type\": \"text\"},
                      \"region\": {\"value\": \"auto\", \"type\": \"text\"},
                      \"number_of_workers\": {\"value\": 3, \"type\": \"integer\"},
                      \"interval\": {\"value\": \"1m\", \"type\": \"text\"},
                      \"preserve_original_event\": {\"value\": false, \"type\": \"bool\"},
                      \"preserve_duplicate_custom_fields\": {\"value\": false, \"type\": \"bool\"},
                      \"tags\": {\"value\": [\"forwarded\", \"cloudflare_logpush-${STREAM}\"], \"type\": \"text\"}
                    }
                  }]
                }"
              done

              INPUTS="${INPUTS}]"

              # Also disable all non-S3 inputs (httpjson, gcs, azure-blob-storage)
              for STREAM in ${STREAMS}; do
                for INPUT_TYPE in httpjson gcs azure-blob-storage; do
                  INPUTS="${INPUTS%]},"
                  INPUTS="${INPUTS}{
                    \"type\": \"${INPUT_TYPE}\",
                    \"policy_template\": \"${STREAM}\",
                    \"enabled\": false,
                    \"streams\": [{
                      \"enabled\": false,
                      \"data_stream\": {
                        \"type\": \"logs\",
                        \"dataset\": \"cloudflare_logpush.${STREAM}\"
                      }
                    }]
                  }]"
                done
              done

              echo "Creating package policy with all 13 data streams..."
              RESULT=$(curl -sk -u "${AUTH}" -X POST "${KIBANA}/api/fleet/package_policies" \
                -H "kbn-xsrf: true" -H "Content-Type: application/json" \
                -d "{
                  \"name\": \"Cloudflare Logpush - R2\",
                  \"namespace\": \"default\",
                  \"policy_id\": \"eck-fleet-server\",
                  \"package\": {
                    \"name\": \"cloudflare_logpush\",
                    \"version\": \"1.43.2\"
                  },
                  \"inputs\": ${INPUTS}
                }")
              echo "${RESULT}" | head -c 500
              echo ""

              # Check if it succeeded
              if echo "${RESULT}" | grep -q '"item"'; then
                echo "SUCCESS: Cloudflare Logpush integration configured"
              else
                echo "WARN: Check the output above for errors"
                # Try to get more details
                echo "${RESULT}" | tail -c 500
              fi
          env:
            - name: ES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elasticsearch-es-elastic-user
                  key: elastic
